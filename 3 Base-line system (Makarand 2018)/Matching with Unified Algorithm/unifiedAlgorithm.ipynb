{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "# for MIR-QBSH query\n",
    "def read_semitone_from_MIR_query(dir_query):\n",
    "    list_query_name = []\n",
    "    df_query = []\n",
    "    truth = []\n",
    "\n",
    "    for file in os.listdir(dir_query):\n",
    "        if file.endswith(\".csv\"):\n",
    "            list_query_name.append(file.replace(\".csv\",\"\"))\n",
    "            file_path = os.path.join(dir_query, file)\n",
    "            fields = [\"semitone\"]\n",
    "            temp_df = pd.read_csv(file_path, usecols=fields)\n",
    "            df_query.append(temp_df.to_numpy().flatten())\n",
    "            truth.append(file.split(\"-\")[2])\n",
    "\n",
    "    return list_query_name, df_query, truth\n",
    "\n",
    "# for MIR-QBSH manually labelled query\n",
    "def read_semitone_from_MIR_manual_query(dir_query):\n",
    "    list_query_name = []\n",
    "    df_query = []\n",
    "    truth = []\n",
    "\n",
    "    for file in os.listdir(dir_query):\n",
    "        if file.endswith(\".pv\"):\n",
    "            list_query_name.append(file.replace(\".pv\",\"\"))\n",
    "            file_path = os.path.join(dir_query, file)\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            temp_df = temp_df.to_numpy().flatten()\n",
    "            round_df = []\n",
    "            for i in temp_df:\n",
    "                if i!=0:\n",
    "                    round_df.append(round(i))\n",
    "            df_query.append(round_df)\n",
    "            truth.append(file.replace(\".pv\",\"\").split(\"-\")[2])\n",
    "\n",
    "    return list_query_name, df_query, truth\n",
    "\n",
    "# for IOACAS_QBH query\n",
    "def read_semitone_from_IOACAS_query(dir_query):\n",
    "    list_query_name = []\n",
    "    df_query = []\n",
    "    truth = []\n",
    "\n",
    "    truth_file = os.path.join(dir_query, \"query_truth.list\")\n",
    "\n",
    "    # read csv as string type need converters\n",
    "    truth_df = pd.read_csv(truth_file, header=None, sep=\"\\t\", converters={i: str for i in range(100)})\n",
    "\n",
    "    all_truth = truth_df[1].values.astype(str).flatten()\n",
    "    # print(\"all truth\", all_truth)\n",
    "    all_wav = []\n",
    "\n",
    "    for wav in truth_df[0]:\n",
    "        all_wav.append(wav.split(\"\\\\\")[1].replace(\".wav\",\"\"))\n",
    "\n",
    "    for file in os.listdir(dir_query):\n",
    "        if file.endswith(\".csv\"):\n",
    "            list_query_name.append(file.replace(\".csv\",\"\"))\n",
    "            file_path = os.path.join(dir_query, file)\n",
    "            fields = [\"semitone\"]\n",
    "            temp_df = pd.read_csv(file_path, usecols=fields)\n",
    "            df_query.append(temp_df.to_numpy().flatten())\n",
    "            # index_truth = all_wav.index(\"002_010\")\n",
    "            index_truth = all_wav.index(file.split(\"-\")[0])\n",
    "            truth.append(str(all_truth[index_truth]))\n",
    "    # print(\"one truth\", truth)\n",
    "\n",
    "    return list_query_name, df_query, truth\n",
    "\n",
    "# for MIR-QBSH and IOACAS-QBH database\n",
    "def read_note_from_midi(dir_midi):\n",
    "    pickled_semitone = \"pickled_\"+dir_midi.replace(folder_data+\"\\\\\",\"\")\n",
    "    dir_pickle = os.path.join(folder_pickle,pickled_semitone)\n",
    "\n",
    "    if os.path.exists(dir_pickle):\n",
    "        open_file = open(dir_pickle, \"rb\")\n",
    "        list_dir_midi, df_midi = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "    else:\n",
    "        open_file = open(dir_pickle, \"wb\")\n",
    "        list_dir_midi = []\n",
    "        df_midi = []\n",
    "\n",
    "        for file in os.listdir(dir_midi):\n",
    "            if file.endswith(\".csv\"):\n",
    "                list_dir_midi.append(file.replace(\".mid.csv\",\"\"))\n",
    "                file_path = os.path.join(dir_midi, file)\n",
    "                fields = [\"note_index\"]\n",
    "                temp_df = pd.read_csv(file_path, usecols=fields)\n",
    "                df_midi.append(temp_df.to_numpy().flatten())\n",
    "        pickle.dump([list_dir_midi, df_midi], open_file)\n",
    "        open_file.close()\n",
    "\n",
    "    return list_dir_midi, df_midi\n",
    "\n",
    "# calculate relative distance\n",
    "def calc_rel_dis(df):\n",
    "    res = []\n",
    "    length = len(df)\n",
    "    for i in range(length-1):\n",
    "        dis = float(df[i+1] - df[i])\n",
    "        res.append(dis)\n",
    "\n",
    "#     ada di eksperimen 1, tapi kayaknya nggak masuk akal kalau dipake\n",
    "#     res = remove_consecutive(res)\n",
    "\n",
    "    return res\n",
    "\n",
    "# remove consecutive distance in list of relative distance \n",
    "def remove_consecutive(list, isQuery):\n",
    "    if isQuery == True:\n",
    "        if \"DNN-LSTM\" in dir_query:\n",
    "            # eksperimen untuk DNN-LSTM\n",
    "            list = get_only_n_consecutive_notes(list, 5)\n",
    "\n",
    "        else:\n",
    "            # eksperimen 1, 3\n",
    "            # list = remove_only_consecutive(list)\n",
    "\n",
    "            # eksperimen 2, 5, 6\n",
    "            list = get_only_10_consecutive_notes(list)\n",
    "                \n",
    "            # eksperimen 4\n",
    "            # list = get_exact_10_consecutive_notes(list)\n",
    "        \n",
    "    else:\n",
    "        # eksperimen 2 dan 4\n",
    "        # do nothing\n",
    "        \n",
    "        # eksperimen 1, 3, 5\n",
    "        list = remove_only_consecutive(list)\n",
    "\n",
    "    return list\n",
    "\n",
    "import numpy as np\n",
    "def remove_only_consecutive(ls):\n",
    "    i = 0\n",
    "    while i < len(ls)-1:\n",
    "        if ls[i] == ls[i+1]:\n",
    "            ls = np.delete(ls, i)\n",
    "        else:\n",
    "            i = i+1\n",
    "    return ls\n",
    "\n",
    "from itertools import groupby\n",
    "def get_only_10_consecutive_notes(ls):\n",
    "    res = []\n",
    "    n = 10\n",
    "    for k, v in groupby(ls):\n",
    "        value = list(v)\n",
    "        if len(value) >= n:\n",
    "            res.append(value[0])\n",
    "    return res\n",
    "\n",
    "from itertools import groupby\n",
    "def get_only_n_consecutive_notes(ls, n):\n",
    "    res = []\n",
    "    for k, v in groupby(ls):\n",
    "        value = list(v)\n",
    "        if len(value) >= n:\n",
    "            res.append(value[0])\n",
    "    return res\n",
    "\n",
    "def get_exact_10_consecutive_notes(list):\n",
    "    res = []\n",
    "    i = 0\n",
    "    while i < len(list)-9:\n",
    "        cur = list[i]\n",
    "        marker = True\n",
    "        for j in range(1,10):\n",
    "            if cur != list[i+j]:\n",
    "                marker = False\n",
    "                i+=j-1\n",
    "                break\n",
    "        if marker == True:\n",
    "            res.append(cur)\n",
    "            i+=10\n",
    "        else:\n",
    "            i+=1\n",
    "            \n",
    "    return res\n",
    "\n",
    "# arrange index according to its similarity, more similar is less in index\n",
    "def get_all_rank_max(ls):\n",
    "    res = []\n",
    "    for i in ls:\n",
    "        temp = ls.index(max(ls))\n",
    "        res.append(list_dir_midi[temp])\n",
    "        ls[temp] = 0\n",
    "    return res\n",
    "\n",
    "# arrange index according to its distance, less distance is less in index\n",
    "def get_all_rank_min(ls):\n",
    "    res = []\n",
    "    for i in ls:\n",
    "        temp = ls.index(min(ls))\n",
    "        res.append(list_dir_midi[temp])\n",
    "        ls[temp] = sys.maxsize\n",
    "    return res\n",
    "\n",
    "# get rank from ground truth\n",
    "def get_rank(rank, truth):\n",
    "    try:\n",
    "        return rank.index(truth)+1\n",
    "    except ValueError as error:\n",
    "        return sys.maxsize\n",
    "    \n",
    "# get top ten rank\n",
    "def get_top_ten(rank):\n",
    "    return rank[:10]\n",
    "\n",
    "# get inverted index from all midi\n",
    "def get_inverted_index_midi(dis_midi, list_dir_midi):\n",
    "    # create inverted index for 2-grams, 3-grams, 4-grams in all midi\n",
    "    import hashedindex\n",
    "    RP4G = hashedindex.HashedIndex()\n",
    "    RP3G = hashedindex.HashedIndex()\n",
    "    RP2G = hashedindex.HashedIndex()\n",
    "\n",
    "    # example:\n",
    "    # midi relative distance : +1 +1 +4\n",
    "    # 2-grams are +1 and +4\n",
    "    # 3-grams are +1 +1 and +1 +3\n",
    "    # 4-grams is +1 +1 +4\n",
    "\n",
    "    # print(dis_midi[0])\n",
    "\n",
    "    # inserting 2-grams as inverted index\n",
    "    for midiNumber in range(len(dis_midi)):\n",
    "        for note in dis_midi[midiNumber]:\n",
    "            RP2G.add_term_occurrence(note, list_dir_midi[midiNumber])\n",
    "\n",
    "    # test 2-grams\n",
    "    # RP2G.get_documents((2.0))\n",
    "\n",
    "    # inserting 3-grams as inverted index\n",
    "    for midiNumber in range(len(dis_midi)):\n",
    "        for noteNumber in range(len(dis_midi[midiNumber])-1):\n",
    "            term = (dis_midi[midiNumber][noteNumber], dis_midi[midiNumber][noteNumber+1])\n",
    "            RP3G.add_term_occurrence(term, list_dir_midi[midiNumber])\n",
    "\n",
    "    # test 3-grams\n",
    "    # RP3G.get_documents((2.0, 0.0))\n",
    "\n",
    "    # inserting 4-grams as inverted index\n",
    "    for midiNumber in range(len(dis_midi)):\n",
    "        for noteNumber in range(len(dis_midi[midiNumber])-2):\n",
    "            term = (dis_midi[midiNumber][noteNumber], \n",
    "            dis_midi[midiNumber][noteNumber+1], dis_midi[midiNumber][noteNumber+2])\n",
    "            \n",
    "            RP4G.add_term_occurrence(term, list_dir_midi[midiNumber])\n",
    "\n",
    "    # test 4-grams\n",
    "    # RP4G.get_documents((2.0, 0.0, -2.0))\n",
    "\n",
    "    return RP4G, RP3G, RP2G\n",
    "\n",
    "# get lists containing 4-grams, 3-grams, and 2-grams from each query\n",
    "def get_ngrams_query(dis_query):\n",
    "    RP4G = []\n",
    "    RP3G = []\n",
    "    RP2G = []\n",
    "\n",
    "    # inserting list of 4-grams\n",
    "    for noteNumber in range(len(dis_query)-2):\n",
    "        term = (dis_query[noteNumber], dis_query[noteNumber+1], dis_query[noteNumber+2])\n",
    "        RP4G.append(term)\n",
    "\n",
    "    # inserting list of 3-grams\n",
    "    for noteNumber in range(len(dis_query)-1):\n",
    "        term = (dis_query[noteNumber], dis_query[noteNumber+1])\n",
    "        RP3G.append(term)\n",
    "\n",
    "    # inserting list of 2-grams\n",
    "    RP2G = dis_query\n",
    "    \n",
    "    return RP4G, RP3G, RP2G\n",
    "\n",
    "from collections import Counter\n",
    "# get Counter containing number of query appearances in inverted index\n",
    "def get_counter_grams(index, query):\n",
    "    res = Counter()\n",
    "    for grams in query:\n",
    "        try:\n",
    "            res += index.get_documents(grams)\n",
    "        except IndexError as error:\n",
    "            pass\n",
    "    return res\n",
    "\n",
    "# get rank from counter grams\n",
    "def get_rank_from_counter_grams(res_grams, truth):\n",
    "    rank = []\n",
    "    for i in res_grams.most_common():\n",
    "        rank.append(i[0])\n",
    "    return rank\n",
    "\n",
    "def get_rank_truth_from_counter_grams(rank, truth):\n",
    "    try:\n",
    "        rankTruth = get_rank(rank, truth)\n",
    "    except ValueError as error:\n",
    "        rankTruth = sys.maxsize\n",
    "    return rankTruth\n",
    "\n",
    "# get rank with relative pitch 4-grams (RP4G), 3-grams (RP3G) and 2-grams (RP2G)\n",
    "def get_rank_with_rpg(dis_query, truth, index_midi):\n",
    "    index4G, index3G, index2G = index_midi\n",
    "    query4G, query3G, query2G = get_ngrams_query(dis_query)\n",
    "\n",
    "    # 4-grams\n",
    "    res_4grams = get_counter_grams(index4G, query4G)\n",
    "\n",
    "    # 3-grams\n",
    "    res_3grams = get_counter_grams(index3G, query3G)\n",
    "\n",
    "    # 2-grams\n",
    "    res_2grams = get_counter_grams(index2G, query2G)\n",
    "\n",
    "    res_rank = []\n",
    "    res_rankTruth = []\n",
    "\n",
    "    rank = get_rank_from_counter_grams(res_4grams, truth)\n",
    "    topTen = get_top_ten(rank)\n",
    "    res_rank.append(topTen)\n",
    "    res_rankTruth.append(get_rank_truth_from_counter_grams(rank, truth))\n",
    "\n",
    "    rank = get_rank_from_counter_grams(res_3grams, truth)\n",
    "    topTen = get_top_ten(rank)\n",
    "    res_rank.append(topTen)\n",
    "    res_rankTruth.append(get_rank_truth_from_counter_grams(rank, truth))\n",
    "\n",
    "    rank = get_rank_from_counter_grams(res_2grams, truth)\n",
    "    topTen = get_top_ten(rank)\n",
    "    res_rank.append(topTen)\n",
    "    res_rankTruth.append(get_rank_truth_from_counter_grams(rank, truth))\n",
    "\n",
    "    return res_rankTruth, res_rank\n",
    "\n",
    "def subsequent_MNF(ref, hyp):    \n",
    "    res = 0\n",
    "    if len(hyp)>len(ref):\n",
    "        hyp, ref = ref, hyp\n",
    "    \n",
    "    diff = len(ref) - len(hyp)\n",
    "    for i in range(diff+1):\n",
    "        subRef = ref[i:i+len(hyp)]\n",
    "        ratio = edit_distance.SequenceMatcher(a=subRef, b=hyp).ratio()\n",
    "        if (res<ratio):\n",
    "            res = ratio\n",
    "\n",
    "    return res\n",
    "\n",
    "# get rank with Mode Normalised Frequency using edit distance method\n",
    "def get_rank_with_mnf(df_query, truth, midi_MNF):\n",
    "    # ref = dis_midi\n",
    "    # hyp = dis_query\n",
    "\n",
    "    hyp = convert_df_to_MNF(df_query)\n",
    "\n",
    "    list_ratio = []\n",
    "    for ref in midi_MNF:\n",
    "        sm = edit_distance.SequenceMatcher(a=ref, b=hyp)\n",
    "        list_ratio.append(sm.ratio())\n",
    "#         ratio = subsequent_MNF(ref, hyp)\n",
    "#         list_ratio.append(ratio)\n",
    "    \n",
    "    rank = get_all_rank_max(list_ratio)\n",
    "    rankTruth = get_rank(rank, truth)\n",
    "\n",
    "    # show top ten result\n",
    "    topTen = get_top_ten(rank)\n",
    "    # print(topTen)\n",
    "\n",
    "    return rankTruth, topTen\n",
    "\n",
    "# get best rank from both nGrams and edit distance method\n",
    "def get_rank_with_unified_algorithm(dis_query, df_query, truth, index_midi, midi_MNF):\n",
    "    rank = []\n",
    "    topTen = []\n",
    "\n",
    "    temp_rank, temp_topTen = get_rank_with_rpg(dis_query, truth, index_midi)\n",
    "    for r in temp_rank:\n",
    "        rank.append(r)\n",
    "    for t in temp_topTen:\n",
    "        topTen.append(t)\n",
    "\n",
    "    try:\n",
    "        temp_rank, temp_topTen = get_rank_with_mnf(df_query, truth, midi_MNF)\n",
    "        rank.append(temp_rank)\n",
    "        topTen.append(temp_topTen)\n",
    "    except ValueError as error:\n",
    "        pass\n",
    "    except IndexError as error:\n",
    "        pass\n",
    "\n",
    "    bestIndex = rank.index(min(rank))\n",
    "    bestRank = rank[bestIndex]\n",
    "    bestTopTen = topTen[bestIndex]\n",
    "\n",
    "    # to return which algorithm: RP4G, RP3G, RP2G, or MNF\n",
    "    switcher = {\n",
    "        0: \"RP4G\",\n",
    "        1: \"RP3G\",\n",
    "        2: \"RP2G\",\n",
    "        3: \"MNF\"\n",
    "    }\n",
    "    alg = switcher.get(bestIndex)\n",
    "\n",
    "    return bestRank, bestTopTen, alg\n",
    "\n",
    "# convert one dataframe containing semitone to Mode Normalised Frequency (MNF)\n",
    "def convert_df_to_MNF(df):\n",
    "    from collections import Counter\n",
    "    res = []\n",
    "    data = Counter(df)\n",
    "    \n",
    "    data = data.most_common()\n",
    "    mode = data[0][0]\n",
    "    adder = int(78-mode) # mode marked as char 'N' which is 78 in ascii\n",
    "\n",
    "    for i in df:\n",
    "        res.append(chr(int(i+adder)))\n",
    "        # res.append(i) # to test matching with no MNF\n",
    "\n",
    "    return res\n",
    "\n",
    "# count MRR from a list of rank from some queries\n",
    "def count_MRR(list_rank):\n",
    "    mrr = 0\n",
    "    counter = 0\n",
    "    for rank in list_rank:\n",
    "        if not rank == sys.maxsize:\n",
    "            mrr+=1/rank\n",
    "            counter+=1\n",
    "    return round(mrr/counter,2), counter\n",
    "\n",
    "# count top 1 hit ratios from list rank\n",
    "def count_top_1_ratio(list_rank):\n",
    "    count_top1 = 0\n",
    "    counter = 0\n",
    "    for rank in list_rank:\n",
    "        if not rank == sys.maxsize:\n",
    "            if rank == 1:\n",
    "                count_top1+=1\n",
    "            counter+=1\n",
    "    return round(count_top1/counter,2), counter\n",
    "\n",
    "# count top 3 hit ratios from list rank\n",
    "def count_top_3_ratio(list_rank):\n",
    "    count_top3 = 0\n",
    "    counter = 0\n",
    "    for rank in list_rank:\n",
    "        if not rank == sys.maxsize:\n",
    "            if rank <= 3:\n",
    "                count_top3+=1\n",
    "            counter+=1\n",
    "    return round(count_top3/counter,2), counter\n",
    "\n",
    "# count top 5 hit ratios from list rank\n",
    "def count_top_5_ratio(list_rank):\n",
    "    count_top5 = 0\n",
    "    counter = 0\n",
    "    for rank in list_rank:\n",
    "        if not rank == sys.maxsize:\n",
    "            if rank <= 5:\n",
    "                count_top5+=1\n",
    "            counter+=1\n",
    "    return round(count_top5/counter,2), counter\n",
    "\n",
    "# count top 10 hit ratios from list rank\n",
    "def count_top_10_ratio(list_rank):\n",
    "    count_top10 = 0\n",
    "    counter = 0\n",
    "    for rank in list_rank:\n",
    "        if not rank == sys.maxsize:\n",
    "            if rank <= 10:\n",
    "                count_top10+=1\n",
    "            counter+=1\n",
    "    return round(count_top10/counter,2), counter\n",
    "\n",
    "def print_result_to_file(process_name):\n",
    "    from datetime import datetime\n",
    "\n",
    "    dateTimeObj = str(datetime.now())+\".txt\"\n",
    "    dateTimeObj = dateTimeObj.replace(\":\",\"-\")\n",
    "    filename = os.path.join(folder_hasil, process_name + ' ' + dateTimeObj)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(cap.stdout)\n",
    "\n",
    "from tslearn.metrics import dtw_subsequence_path as dtw\n",
    "def get_rank_with_dtw(df_query, df_midi, truth):\n",
    "    list_dist = []\n",
    "    \n",
    "    if \"DNN-LSTM\" in dir_query:\n",
    "        # eksperimen untuk DNN-LSTM\n",
    "        query = get_only_n_consecutive_notes(df_query, 5)\n",
    "\n",
    "    else:\n",
    "        # eksperimen 6\n",
    "        query = get_only_10_consecutive_notes(df_query)\n",
    "\n",
    "    if len(query)==0:\n",
    "        return sys.maxsize\n",
    "\n",
    "    for midi in df_midi:\n",
    "        midi = remove_only_consecutive(midi)\n",
    "        path, dist = dtw(query, midi)\n",
    "        list_dist.append(dist)\n",
    "\n",
    "    rank = get_all_rank_min(list_dist)\n",
    "\n",
    "    rankTruth = get_rank(rank, truth)\n",
    "\n",
    "    return rankTruth\n",
    "\n",
    "def do_matching_process(dis_query, c_query, truth, index_midi, midi_MNF):\n",
    "    list_rank = []\n",
    "    list_time = []\n",
    "\n",
    "    for i in range(len(c_query)):\n",
    "        print(\"Processing...\",i+1,\"/\",len(c_query))\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        # eksperimen 1,2,3,4,5\n",
    "        rankTruth, topTen, alg = get_rank_with_unified_algorithm(dis_query[i], c_query[i], truth[i], index_midi, midi_MNF)\n",
    "\n",
    "        # eksperimen 6\n",
    "        # rankTruth = get_rank_with_dtw(c_query[i], df_midi, truth[i])\n",
    "\n",
    "        # show ground truth from a query\n",
    "        # print(\"query\",list_query_name[i],\"truth is\",truth[i])\n",
    "\n",
    "        found = rankTruth != sys.maxsize\n",
    "\n",
    "        if found == True:\n",
    "            list_time.append(round(time.time() - start_time, 3))\n",
    "            list_rank.append(rankTruth)\n",
    "\n",
    "            print(\"query\",list_query_name[i],\"truth is on rank\",rankTruth)\n",
    "            # print(\"top ten result from\",alg,\":\",topTen)\n",
    "        else:\n",
    "            print(\"query\",list_query_name[i],\"truth is not found on database\")\n",
    "\n",
    "    return list_rank, list_time\n",
    "\n",
    "def process_query(dir_query, dir_midi, list_query_name, df_query, truth, list_dir_midi, df_midi):\n",
    "    # get compressed query and midi by removing consecutive notes\n",
    "    c_query = []\n",
    "    for query in df_query:\n",
    "        compressed_query = remove_consecutive(query, isQuery = True)\n",
    "        c_query.append(compressed_query)\n",
    "\n",
    "    c_midi = []\n",
    "    for midi in df_midi:\n",
    "        compressed_midi = remove_consecutive(midi, isQuery=False)\n",
    "        c_midi.append(compressed_midi)\n",
    "\n",
    "    # calculate relative distance in all query and midi\n",
    "    dis_query, dis_midi = get_rel_dis_query_midi(c_query, c_midi)\n",
    "\n",
    "    index_midi = get_inverted_index_midi(dis_midi, list_dir_midi)\n",
    "\n",
    "    midi_MNF = []\n",
    "    for i in c_midi:\n",
    "        temp = convert_df_to_MNF(i)\n",
    "        midi_MNF.append(temp)\n",
    "\n",
    "    print(\"Start matching process\")\n",
    "\n",
    "    list_rank, list_time = do_matching_process(dis_query, c_query, truth, index_midi, midi_MNF)\n",
    "\n",
    "    try:\n",
    "        mrr, counter = count_MRR(list_rank)\n",
    "        print(\"MRR:\", mrr, \"from\", counter, \"queries\")\n",
    "\n",
    "        hit_ratio, counter = count_top_1_ratio(list_rank)\n",
    "        print(\"Top 1 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_3_ratio(list_rank)\n",
    "        print(\"Top 3 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_5_ratio(list_rank)\n",
    "        print(\"Top 5 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_10_ratio(list_rank)\n",
    "        print(\"Top 10 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "        \n",
    "        print(\"Average time for matching:\", round(sum(list_time)/len(list_time), 3),\"s\")\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        print(\"No result to show\")\n",
    "        # print(\"list rank\", list_rank)\n",
    "        # print(\"list time\", list_time)\n",
    "\n",
    "    print(\"Finish matching process\")\n",
    "\n",
    "def process_query_uncontinue(dir_query, dir_midi, list_query_name, df_query, truth, list_dir_midi, df_midi):\n",
    "    df_query_firstHalf = []\n",
    "    df_query_secondHalf = []\n",
    "    \n",
    "    for query in df_query:\n",
    "        firstHalf = query[:len(query)//2]\n",
    "        secondHalf = query[len(query)//2:]\n",
    "        df_query_firstHalf.append(firstHalf)\n",
    "        df_query_secondHalf.append(secondHalf)\n",
    "\n",
    "    # get compressed query and midi by removing consecutive notes\n",
    "    c_query_firstHalf = []\n",
    "    for query in df_query_firstHalf:\n",
    "        compressed_query = remove_consecutive(query, isQuery = True)\n",
    "        c_query_firstHalf.append(compressed_query)\n",
    "\n",
    "    c_query_secondHalf = []\n",
    "    for query in df_query_secondHalf:\n",
    "        compressed_query = remove_consecutive(query, isQuery = True)\n",
    "        c_query_secondHalf.append(compressed_query)\n",
    "\n",
    "    c_midi = []\n",
    "    for midi in df_midi:\n",
    "        compressed_midi = remove_consecutive(midi, isQuery=False)\n",
    "        c_midi.append(compressed_midi)\n",
    "\n",
    "    # calculate relative distance in all query and midi\n",
    "    dis_query_firstHalf, dis_midi = get_rel_dis_query_midi(c_query_firstHalf, c_midi)\n",
    "    dis_query_secondHalf, dis_midi = get_rel_dis_query_midi(c_query_secondHalf, c_midi)\n",
    "\n",
    "    index_midi = get_inverted_index_midi(dis_midi, list_dir_midi)\n",
    "\n",
    "    midi_MNF = []\n",
    "    for i in c_midi:\n",
    "        temp = convert_df_to_MNF(i)\n",
    "        midi_MNF.append(temp)\n",
    "\n",
    "    print(\"Start matching process\")\n",
    "\n",
    "    print(\"Start matching for first half\")\n",
    "    list_rank_firstHalf, list_time_firstHalf = do_matching_process(dis_query_firstHalf, c_query_firstHalf, truth, index_midi, midi_MNF)\n",
    "    print(\"Finish matching for first half\")\n",
    "\n",
    "    print(\"Start matching for second half\")\n",
    "    list_rank_secondHalf, list_time_secondHalf = do_matching_process(dis_query_secondHalf, c_query_secondHalf, truth, index_midi, midi_MNF)\n",
    "    print(\"Finish matching for second half\")\n",
    "\n",
    "    list_best_rank = []\n",
    "    list_total_time = []\n",
    "\n",
    "    for counter in range(len(list_rank_firstHalf)):\n",
    "        if list_rank_firstHalf[counter]>list_rank_secondHalf[counter]:\n",
    "            temp_rank = list_rank_secondHalf[counter]\n",
    "        elif list_rank_firstHalf[counter]<list_rank_secondHalf[counter]:\n",
    "            temp_rank = list_rank_firstHalf[counter]\n",
    "        else:\n",
    "            temp_rank = list_rank_firstHalf[counter]\n",
    "\n",
    "        total_time = list_time_firstHalf[counter]+list_time_secondHalf[counter]\n",
    "\n",
    "        list_best_rank.append(temp_rank)\n",
    "        list_total_time.append(total_time)\n",
    "\n",
    "    try:\n",
    "        mrr, counter = count_MRR(list_best_rank)\n",
    "        print(\"MRR:\", mrr, \"from\", counter, \"queries\")\n",
    "\n",
    "        hit_ratio, counter = count_top_1_ratio(list_best_rank)\n",
    "        print(\"Top 1 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_3_ratio(list_best_rank)\n",
    "        print(\"Top 3 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_5_ratio(list_best_rank)\n",
    "        print(\"Top 5 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "\n",
    "        hit_ratio, counter = count_top_10_ratio(list_best_rank)\n",
    "        print(\"Top 10 ratio:\", hit_ratio, \"from\", counter, \"queries\" )\n",
    "        \n",
    "        print(\"Average time for matching:\", round(sum(list_total_time)/len(list_total_time), 3),\"s\")\n",
    "    \n",
    "    except ZeroDivisionError:\n",
    "        print(\"No result to show\")\n",
    "        # print(\"list rank\", list_best_rank)\n",
    "        # print(\"list time\", list_total_time)\n",
    "\n",
    "    print(\"Finish matching process\")\n",
    "\n",
    "import math\n",
    "# convert pitch to semitone\n",
    "def convert_f0_to_semitone(f0):\n",
    "    res = []\n",
    "    for freq in f0:\n",
    "        res.append(round(12*math.log((freq/440),2)+69))\n",
    "    return res\n",
    "\n",
    "# extract pitch and semitone with parselmouth from wav to csv\n",
    "def extract_pitch_with_parselmouth_to_csv(folder, dir_out):\n",
    "    import parselmouth\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder, file)\n",
    "\n",
    "            snd = parselmouth.Sound(file_path)\n",
    "\n",
    "            pitch = snd.to_pitch()\n",
    "            pitch_values = pitch.selected_array['frequency']\n",
    "\n",
    "            time = []\n",
    "            f0 = []\n",
    "\n",
    "            for i in range(len(pitch_values)):\n",
    "                if pitch_values[i]!=0:\n",
    "                    f0.append(pitch_values[i].round(decimals=3))\n",
    "                    time.append(pitch.xs()[i].round(decimals=3))\n",
    "\n",
    "            file = file_path.split(\"\\\\\")\n",
    "            file_csv = os.path.join(dir_out, file[2] + \"-parselmouth.csv\")\n",
    "            file_csv = file_csv.replace(\".wav\",\"\")\n",
    "\n",
    "            semitone = convert_f0_to_semitone(f0)\n",
    "\n",
    "            pd.DataFrame({'time':time, 'semitone':semitone, 'f0':f0}).to_csv(file_csv, index=False)\n",
    "\n",
    "# extract pitch and semitone with DNN-LSTM from wav to csv\n",
    "def extract_pitch_with_DNN_LSTM_to_csv(folder, dir_out):\n",
    "    import shutil\n",
    "    \n",
    "    folder_project_DNN_LSTM = \"Lab_MelExt\"\n",
    "\n",
    "    dir_temp_in = os.path.join(folder_project_DNN_LSTM, \"temp_query\")\n",
    "\n",
    "    if not os.path.exists(dir_temp_in):\n",
    "        os.mkdir(dir_temp_in)\n",
    "    else:\n",
    "        delete_folder_content(dir_temp_in)\n",
    "\n",
    "    # copy query from main folder to DNN-LSTM project folder\n",
    "    for i in os.listdir(folder):\n",
    "        source = os.path.join(folder, i)\n",
    "        destination = os.path.join(dir_temp_in, i)\n",
    "        shutil.copy(source, destination)\n",
    "\n",
    "    dir_temp_query = os.path.join(folder_project_DNN_LSTM, \"temp_pitch\")\n",
    "    \n",
    "    if os.path.exists(dir_temp_query):\n",
    "        delete_folder_content(dir_temp_query)\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    os.chdir(folder_project_DNN_LSTM)\n",
    "    cmd_line = \"python predict.py temp_query temp_pitch\"\n",
    "    os.system(cmd_line)\n",
    "    os.chdir(cwd)\n",
    "\n",
    "    for file in os.listdir(dir_temp_query):\n",
    "        if file.endswith(\".csv\"):\n",
    "            filePath = os.path.join(dir_temp_query, file)\n",
    "            df = pd.read_csv(filePath, sep=\"\\t\", header=None)\n",
    "\n",
    "            time = []\n",
    "            f0 = []\n",
    "\n",
    "            for i in range(len(df[1])):\n",
    "                if df[1][i]!=0:\n",
    "                    time.append(df[0][i].round(decimals=3))\n",
    "                    f0.append(df[1][i].round(decimals=3))\n",
    "                    \n",
    "            semitone = convert_f0_to_semitone(f0)\n",
    "\n",
    "            file_csv = os.path.join(dir_out, file)\n",
    "\n",
    "            pd.DataFrame({'time':time, 'semitone':semitone, 'f0':f0}).to_csv(file_csv, index=False)\n",
    "    \n",
    "    shutil.rmtree(dir_temp_in)\n",
    "    shutil.rmtree(dir_temp_query)\n",
    "\n",
    "def extract_pitch_to_csv(extractor, folder, dir_out):\n",
    "    print(\"Start melody extraction with\",extractor)\n",
    "    start_time = time.time()\n",
    "    if extractor == \"parselmouth\":\n",
    "        extract_pitch_with_parselmouth_to_csv(folder, dir_out)\n",
    "    elif extractor == \"DNN-LSTM\":\n",
    "        extract_pitch_with_DNN_LSTM_to_csv(folder, dir_out)\n",
    "    duration = round(time.time() - start_time, 3)\n",
    "    print(\"Average time for melody extraction:\",round(duration/query_count,3),\"s\")\n",
    "    print(\"Finish melody extraction with\",extractor)\n",
    "\n",
    "# choose midi based on dir_query\n",
    "def get_midi_dir(dir_query):\n",
    "    if \"48\" in dir_query:\n",
    "        dir_midi = os.path.join(folder_data, \"Database midi IOACAS-QBH 48 MIDI\")\n",
    "    elif \"IOACAS\" in dir_query:\n",
    "        dir_midi = os.path.join(folder_data, \"Database midi IOACAS-QBH\")\n",
    "    elif \"Putri\" in dir_query:\n",
    "        dir_midi = os.path.join(folder_data, \"Database midi Putri\")\n",
    "    else:\n",
    "        dir_midi = os.path.join(folder_data, \"Database midi MIR-QBSH\")\n",
    "    \n",
    "    return dir_midi\n",
    "\n",
    "# read and insert semitone from query to list of dataframe\n",
    "def get_semitone_list(dir_query):\n",
    "    pickled_semitone = \"pickled_\"+dir_query.replace(folder_data+\"\\\\\",\"\")\n",
    "    dir_pickle = os.path.join(folder_pickle,pickled_semitone)\n",
    "\n",
    "    if os.path.exists(dir_pickle):\n",
    "        open_file = open(dir_pickle, \"rb\")\n",
    "        list_query_name, df_query, truth = pickle.load(open_file)\n",
    "        open_file.close()\n",
    "    else:\n",
    "        if \"manual\" in dir_query:\n",
    "            list_query_name, df_query, truth = read_semitone_from_MIR_manual_query(dir_query)\n",
    "        elif \"IOACAS\" in dir_query:\n",
    "            list_query_name, df_query, truth = read_semitone_from_IOACAS_query(dir_query)\n",
    "        elif \"Putri\" in dir_query:\n",
    "            list_query_name, df_query, truth = read_semitone_from_IOACAS_query(dir_query)\n",
    "        else:\n",
    "            list_query_name, df_query, truth = read_semitone_from_MIR_query(dir_query)\n",
    "        if not any(value in dir_query for value in (\"Test\", \"test\", \"Temp\", \"temp\", \"Demo\", \"demo\")):\n",
    "            open_file = open(dir_pickle, \"wb\")\n",
    "            pickle.dump([list_query_name, df_query, truth], open_file)\n",
    "            open_file.close()\n",
    "\n",
    "    return list_query_name, df_query, truth\n",
    "\n",
    "# calculate relative distance in all query and midi\n",
    "def get_rel_dis_query_midi(df_query, df_midi):\n",
    "    # calculate relative distance in all query\n",
    "    dis_query = []\n",
    "    for query in df_query:\n",
    "        dis_query.append(calc_rel_dis(query))\n",
    "\n",
    "    # calculate relative distance in all midi\n",
    "    dis_midi = []\n",
    "    for midi in df_midi:\n",
    "        dis_midi.append(calc_rel_dis(midi))\n",
    "\n",
    "    return dis_query, dis_midi\n",
    "\n",
    "def save_to_wav(snd, filename):\n",
    "    if \".wav\" not in filename:\n",
    "        filename = filename + \".wav\"\n",
    "    snd.save(filename, \"WAV\")\n",
    "\n",
    "# code taken from https://dspillustrations.com/pages/posts/misc/fourier-series-and-harmonic-approximation.html\n",
    "def fourierSeries(period, N):\n",
    "    \"\"\"Calculate the Fourier series coefficients up to the Nth harmonic\"\"\"\n",
    "    result = []\n",
    "    T = len(period)\n",
    "    t = np.arange(T)\n",
    "    for n in range(N+1):\n",
    "        an = 2/T*(period * np.cos(2*np.pi*n*t/T)).sum()\n",
    "        bn = 2/T*(period * np.sin(2*np.pi*n*t/T)).sum()\n",
    "        result.append((an, bn))\n",
    "    return np.array(result)\n",
    "\n",
    "# code taken from https://dspillustrations.com/pages/posts/misc/fourier-series-and-harmonic-approximation.html\n",
    "def reconstruct(P, anbn):\n",
    "    result = 0\n",
    "    t = np.arange(P)\n",
    "    for n, (a, b) in enumerate(anbn):\n",
    "        if n == 0:\n",
    "            a = a/2\n",
    "        result += a*np.cos(2*np.pi*n*t/P) + b * np.sin(2*np.pi*n*t/P)\n",
    "    return result\n",
    "\n",
    "import numpy as np\n",
    "def compute_FSD(data):\n",
    "    # assume sample rate = 64000\n",
    "    # with n_frame = 640, each frame will contain 100 samples\n",
    "    # with n_harmonics = 10, each frame will reduce from 100 samples to 10 harmonics\n",
    "    n_frame = 640\n",
    "    n_harmonics = 10\n",
    "\n",
    "    # Framing\n",
    "    data_split = np.array_split(data,n_frame)\n",
    "\n",
    "    new_data = np.array([])\n",
    "    \n",
    "    for frame in data_split:\n",
    "        # Fourier series Expansion with N order\n",
    "        F = fourierSeries(frame, n_harmonics)\n",
    "\n",
    "        # Frame reconstruction\n",
    "        frame_i = reconstruct(len(frame), F[:n_harmonics,:])\n",
    "\n",
    "        # Signal reconstruction\n",
    "        new_data = np.concatenate((new_data, frame_i), axis=None)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "import parselmouth\n",
    "def remove_noise_with_FSD(snd):\n",
    "    data = snd.values[0]\n",
    "    samplerate = snd.sampling_frequency\n",
    "\n",
    "    clean_data = compute_FSD(data)\n",
    "    clean_snd = parselmouth.Sound(values=clean_data,sampling_frequency=samplerate)\n",
    "\n",
    "    return clean_snd\n",
    "\n",
    "# SS v3.0\n",
    "import parselmouth\n",
    "def remove_noise_with_spectral_subtraction(snd):\n",
    "    pitch = snd.to_pitch()\n",
    "    pitch_values = pitch.selected_array['frequency']\n",
    "\n",
    "    final_unvoiced_length = 0\n",
    "    final_unvoiced_start_time = 0.0\n",
    "    final_unvoiced_end_time = 0.0\n",
    "    found_unvoiced = False\n",
    "    \n",
    "    for i in range(len(pitch_values)-1):\n",
    "        temp_unvoiced_length = 0\n",
    "        temp_unvoiced_start_time = pitch.xs()[i].round(decimals=3)\n",
    "\n",
    "        while pitch_values[i] == 0.0 and i < len(pitch_values)-1:\n",
    "            temp_unvoiced_length += 1\n",
    "            i += 1\n",
    "        \n",
    "        temp_unvoiced_end_time = pitch.xs()[i].round(decimals=3)\n",
    "\n",
    "        bandwidth = temp_unvoiced_end_time - temp_unvoiced_start_time\n",
    "        # batas minimal bandwidth praat = 0.0625 (dalam detik)\n",
    "\n",
    "        if temp_unvoiced_length > final_unvoiced_length:\n",
    "            if temp_unvoiced_start_time <= 0.1 and bandwidth > 0.5 :\n",
    "                final_unvoiced_length = temp_unvoiced_length\n",
    "                final_unvoiced_start_time = temp_unvoiced_start_time\n",
    "                final_unvoiced_end_time = temp_unvoiced_end_time\n",
    "                found_unvoiced = True\n",
    "                break\n",
    "            elif found_unvoiced == False:\n",
    "                final_unvoiced_length = temp_unvoiced_length\n",
    "                final_unvoiced_start_time = temp_unvoiced_start_time\n",
    "                final_unvoiced_end_time = temp_unvoiced_end_time\n",
    "\n",
    "    if final_unvoiced_length!=0:\n",
    "        # snd_unvoiced = snd.extract_part(from_time = final_unvoiced_start_time, to_time = final_unvoiced_end_time, \n",
    "        # preserve_times=True)\n",
    "        \n",
    "        snd_noise_removed = parselmouth.praat.call(snd, \"Remove noise\", final_unvoiced_start_time, \n",
    "        final_unvoiced_end_time, 0.025, 80.0, 10000.0, 40.0, \"Spectral subtraction\")\n",
    "\n",
    "        parselmouth.Vector.scale_peak(snd_noise_removed)\n",
    "\n",
    "        # return parselmouth object\n",
    "        return snd_noise_removed\n",
    "    else:\n",
    "        print(\"no unvoiced segment found for\", filename)\n",
    "        return snd\n",
    "\n",
    "def clean_wav_file(folder, dir_out):\n",
    "    print(\"Start cleaning wav file\")\n",
    "    start_time = time.time()\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_in = os.path.join(folder, file)\n",
    "            snd_object = parselmouth.Sound(file_in)\n",
    "            snd_object = remove_noise_with_FSD(snd_object)\n",
    "            snd_object = remove_noise_with_spectral_subtraction(snd_object)\n",
    "            file_out = file_in.replace(folder,dir_out)\n",
    "            file_out = file_out.replace(\".wav\",\"-clean.wav\")\n",
    "            save_to_wav(snd_object, file_out)\n",
    "    duration = round(time.time() - start_time, 3)\n",
    "    print(\"Average time for cleaning:\",round(duration/query_count,3),\"s\")\n",
    "    print(\"Finish cleaning wav file\")\n",
    "\n",
    "import os\n",
    "def delete_folder_content(folder):\n",
    "    content = os.listdir(folder)\n",
    "    for file in content:\n",
    "        filename = os.path.join(folder,file)\n",
    "        os.remove(filename)\n",
    "\n",
    "import shutil\n",
    "def move_IOACAS_ground_truth_list_file(source, destination):\n",
    "    source = os.path.join(source, \"query_truth.list\")\n",
    "    destination = os.path.join(destination, \"query_truth.list\")\n",
    "    shutil.copy(source, destination)\n",
    "\n",
    "def make_uncontinue_query(list_query_name, df_query, truth):\n",
    "    df_query_uncontinue = []\n",
    "    for query in df_query:\n",
    "        temp_query = []\n",
    "        size = len(query)\n",
    "        oneThirdSize = int(size/3)\n",
    "        twoThirdSize = 2*oneThirdSize\n",
    "        for q in query[:oneThirdSize]:\n",
    "            temp_query.append(q)\n",
    "        for q in query[twoThirdSize:size]:\n",
    "            temp_query.append(q)\n",
    "        df_query_uncontinue.append(temp_query)\n",
    "    print(\"uncontinue query\")\n",
    "    return list_query_name, df_query_uncontinue, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "# this cell is for matching only\n",
    "\n",
    "# read midi and query data\n",
    "# insert it into dataframe\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import edit_distance\n",
    "import time\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "folder_data = \"Data query dan midi\"\n",
    "folder_hasil = \"Hasil eksperimen\"\n",
    "folder_pickle = \"pickle\"\n",
    "\n",
    "if not os.path.exists(folder_hasil):\n",
    "    os.mkdir(folder_hasil)\n",
    "\n",
    "if not os.path.exists(folder_pickle):\n",
    "    os.mkdir(folder_pickle)\n",
    "\n",
    "# -------------test query-------------\n",
    "dir_query = os.path.join(folder_data, \"Test query MIR-QBSH_midi-to-wav_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query MIR-QBSH_midi-to-wav_DNN-LSTM\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query MIR-QBSH_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query MIR-QBSH_DNN-LSTM\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query MIR-QBSH_manual label\")\n",
    "\n",
    "# dir_query = os.path.join(folder_data, \"Test query Putri_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query Putri_vokal sama nada_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query Putri_vokal beda nada_parselmouth\")\n",
    "\n",
    "# dir_query = os.path.join(folder_data, \"Test query IOACAS_QBH_48_MIDI_midi to wav_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query IOACAS_QBH_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query IOACAS_QBH_DNN-LSTM\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query IOACAS_QBH_parselmouth_48_MIDI\")\n",
    "# dir_query = os.path.join(folder_data, \"Test query IOACAS_QBH_DNN-LSTM_48_MIDI\")\n",
    "\n",
    "\n",
    "# -------------all query-------------\n",
    "# dir_query = os.path.join(folder_data, \"Query_MIR_QBSH_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_MIR_QBSH_DNN-LSTM\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_MIR_QBSH_manual label\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_IOACAS_QBH_parselmouth\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_IOACAS_QBH_DNN-LSTM\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_IOACAS_QBH_parselmouth_48_MIDI\")\n",
    "# dir_query = os.path.join(folder_data, \"Query_IOACAS_QBH_DNN-LSTM_48_MIDI\")\n",
    "\n",
    "# choose midi based on dir_query\n",
    "dir_midi = get_midi_dir(dir_query)\n",
    "\n",
    "process_name = dir_query.replace(folder_data+\"\\\\\",\"\")+\" with \"+dir_midi.replace(folder_data+\"\\\\\",\"\")\n",
    "print(\"Start process\", process_name)\n",
    "\n",
    "# read and insert semitone from query to list of dataframe\n",
    "list_query_name, df_query, truth = get_semitone_list(dir_query)\n",
    "\n",
    "# makes the query discontinuous\n",
    "list_query_name, df_query, truth = make_uncontinue_query(list_query_name, df_query, truth)\n",
    "\n",
    "# read and insert note_index or semitone from midi to list of dataframe\n",
    "list_dir_midi, df_midi = read_note_from_midi(dir_midi)\n",
    "\n",
    "# test match with query data\n",
    "# process_query(dir_query, dir_midi, list_query_name, df_query, truth, list_dir_midi, df_midi)\n",
    "process_query_uncontinue(dir_query, dir_midi, list_query_name, df_query, truth, list_dir_midi, df_midi)\n",
    "\n",
    "print(\"Finish process\", process_name)\n",
    "\n",
    "# place on different cell\n",
    "# print_result_to_file(process_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start process Wav_Query_MIR_QBSH_test with Database midi MIR-QBSH\n",
      "Number of queries: 2\n",
      "Start cleaning wav file\n",
      "Average time for cleaning: 1.145 s\n",
      "Finish cleaning wav file\n",
      "Start melody extraction with parselmouth\n",
      "Average time for melody extraction: 0.212 s\n",
      "Finish melody extraction with parselmouth\n",
      "Start matching process\n",
      "Processing... 1 / 2\n",
      "query year2003-person00001-00013-clean-parselmouth truth is on rank 12\n",
      "Processing... 2 / 2\n",
      "query year2003-person00001-00014-clean-parselmouth truth is on rank 15\n",
      "MRR: 0.07 from 2 queries\n",
      "Top 1 ratio: 0.0 from 2 queries\n",
      "Top 3 ratio: 0.0 from 2 queries\n",
      "Top 5 ratio: 0.0 from 2 queries\n",
      "Top 10 ratio: 0.0 from 2 queries\n",
      "Average time for matching: 0.261 s\n",
      "Finish matching process\n",
      "Finish process Wav_Query_MIR_QBSH_test with Database midi MIR-QBSH\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "\n",
    "# FOR DEMO PURPOSES\n",
    "# COMBINED ALL PROCESS FROM NOISE FILTRATION, PITCH EXTRACTION, AND MATCHING\n",
    "# PARSELMOUTH AND DNN-LSTM PITCH EXTRACTION\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import edit_distance\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "folder_data = \"Data query dan midi\"\n",
    "folder_hasil = \"Hasil eksperimen\"\n",
    "folder_pickle = \"pickle\"\n",
    "# folder_wav = \"Wav_Query_MIR_QBSH_test\"\n",
    "# folder_wav = \"Wav_Query_IOACAS_QBH_48_MIDI_test\"\n",
    "# folder_wav = \"Wav_Query_IOACAS_QBH_test\"\n",
    "\n",
    "# folder_wav = \"Wav_Query_MIR_QBSH\"\n",
    "# folder_wav = \"Wav_Query_IOACAS_QBH_48_MIDI\"\n",
    "# folder_wav = \"Wav_Query_IOACAS_QBH\"\n",
    "folder_wav = \"Wav_Query_MIR_QBSH_test\"\n",
    "\n",
    "# PARAMETER\n",
    "# use noise filtration or not\n",
    "do_noise_filtration = True\n",
    "# do_noise_filtration = False\n",
    "\n",
    "# pitch extractor\n",
    "extractor = \"parselmouth\"\n",
    "# extractor = \"DNN-LSTM\"\n",
    "\n",
    "if not os.path.exists(folder_hasil):\n",
    "    os.mkdir(folder_hasil)\n",
    "\n",
    "dir_wav_query = os.path.join(folder_data, folder_wav)\n",
    "\n",
    "if \"IOACAS\" in dir_wav_query:\n",
    "    query_count = len(os.listdir(dir_wav_query))-1\n",
    "else:\n",
    "    query_count = len(os.listdir(dir_wav_query))\n",
    "\n",
    "# choose midi based on dir_query\n",
    "dir_midi = get_midi_dir(dir_wav_query)\n",
    "\n",
    "process_name = dir_wav_query.replace(folder_data+\"\\\\\",\"\")+\" with \"+dir_midi.replace(folder_data+\"\\\\\",\"\")\n",
    "print(\"Start process\", process_name)\n",
    "\n",
    "print(\"Number of queries:\", query_count)\n",
    "\n",
    "# start query cleaning or noise filtration\n",
    "\n",
    "if do_noise_filtration == True:\n",
    "    dir_clean_wav_query = dir_wav_query+\"_clean\"\n",
    "\n",
    "    if not os.path.exists(dir_clean_wav_query):\n",
    "        os.mkdir(dir_clean_wav_query)\n",
    "    else:\n",
    "        delete_folder_content(dir_clean_wav_query)\n",
    "\n",
    "    clean_wav_file(dir_wav_query, dir_clean_wav_query)\n",
    "\n",
    "    # copy IOACAS ground truth list file\n",
    "    if \"IOACAS\" in dir_wav_query:\n",
    "        move_IOACAS_ground_truth_list_file(dir_wav_query, dir_clean_wav_query)\n",
    "\n",
    "# end query cleaning or noise filtration\n",
    "\n",
    "# start pitch extraction\n",
    "\n",
    "if do_noise_filtration == True:\n",
    "    dir_wav_query = dir_clean_wav_query\n",
    "\n",
    "dir_query = dir_wav_query+\"_\"+extractor\n",
    "\n",
    "if not os.path.exists(dir_query):\n",
    "    os.mkdir(dir_query)\n",
    "else:\n",
    "    delete_folder_content(dir_query)\n",
    "\n",
    "extract_pitch_to_csv(extractor, dir_wav_query, dir_query)\n",
    "\n",
    "# copy IOACAS ground truth list file\n",
    "if \"IOACAS\" in dir_wav_query:\n",
    "    move_IOACAS_ground_truth_list_file(dir_wav_query, dir_query)\n",
    "    \n",
    "# end pitch extraction\n",
    "\n",
    "# start matching\n",
    "\n",
    "# read and insert semitone from query to list of dataframe\n",
    "list_query_name, df_query, truth = get_semitone_list(dir_query)\n",
    "\n",
    "# read and insert note_index or semitone from midi to list of dataframe\n",
    "list_dir_midi, df_midi = read_note_from_midi(dir_midi)\n",
    "\n",
    "# start query processing and matching with database\n",
    "process_query(dir_query, dir_midi, list_query_name, df_query, truth, list_dir_midi, df_midi)\n",
    "\n",
    "# end matching\n",
    "\n",
    "print(\"Finish process\", process_name)\n",
    "\n",
    "# place on different cell\n",
    "# print_result_to_file(process_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}